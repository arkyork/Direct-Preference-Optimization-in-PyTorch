{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3cfe3327114cfb80efbe7ed162d331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "model=\"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def tokenize_dataset(batch):\n",
    "    # Tokenize prompt, chosen, and rejected\n",
    "    prompt_token = tokenizer(batch['prompt'], padding=True, return_tensors=\"pt\")\n",
    "    chosen_token = tokenizer(batch['chosen'], padding=True, return_tensors=\"pt\")\n",
    "    rejected_token = tokenizer(batch['rejected'], padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Add eos_token\n",
    "    chosen_token['input_ids'] = torch.cat(\n",
    "        [chosen_token['input_ids'], torch.tensor([[tokenizer.eos_token_id]])], dim=1\n",
    "    )\n",
    "    chosen_token['attention_mask'] = torch.cat(\n",
    "        [chosen_token['attention_mask'], torch.tensor([[1]])], dim=1\n",
    "    )\n",
    "\n",
    "    rejected_token['input_ids'] = torch.cat(\n",
    "        [rejected_token['input_ids'], torch.tensor([[tokenizer.eos_token_id]])], dim=1\n",
    "    )\n",
    "    rejected_token['attention_mask'] = torch.cat(\n",
    "        [rejected_token['attention_mask'], torch.tensor([[1]])], dim=1\n",
    "    )\n",
    "\n",
    "    # Concatenate prompt with chosen and rejected\n",
    "    batch['chosen_tokenizer'] = {\n",
    "        'input_ids': torch.cat([prompt_token['input_ids'], chosen_token['input_ids']], dim=1),\n",
    "        'attention_mask': torch.cat([prompt_token['attention_mask'], chosen_token['attention_mask']], dim=1),\n",
    "    }\n",
    "    batch['rejected_tokenizer'] = {\n",
    "        'input_ids': torch.cat([prompt_token['input_ids'], rejected_token['input_ids']], dim=1),\n",
    "        'attention_mask': torch.cat([prompt_token['attention_mask'], rejected_token['attention_mask']], dim=1),\n",
    "    }\n",
    "    batch[\"chosen\"]=batch[\"prompt\"]+batch[\"chosen\"]\n",
    "    batch[\"rejected\"]=batch[\"prompt\"]+batch[\"rejected\"]\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ds=train_dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': ['Oh, I just saw the best meme - have you seen it?',\n",
       "  'Do you have a go-to karaoke jam?'],\n",
       " 'chosen': [\"Oh, I just saw the best meme - have you seen it?😂 Ah, no I haven't! I'm dying to know, what's the meme about? Is it a funny cat or a ridiculous situation? Spill the beans! 🤣\",\n",
       "  'Do you have a go-to karaoke jam?Oh, totally! 😄 I\\'m a sucker for a good ol\\' rock ballad. Give me some Bon Jovi any day of the week! \"Livin\\' on a Prayer\" is my go-to karaoke jam. There\\'s just something about belting out \"Oh, we\\'re halfway there!\" at the top of my lungs that gets me pumped up! 🎤 What about you, do you have a favorite karaoke song? 🎶'],\n",
       " 'rejected': [\"Oh, I just saw the best meme - have you seen it?I'm an artificial intelligence language model, I don't have personal experiences or opinions. However, I can provide you with information on highly-rated and critically acclaimed films, as well as recommendations based on specific genres or themes. Would you like me to suggest some notable movies or discuss a particular genre of interest?\",\n",
       "  \"Do you have a go-to karaoke jam?As a professional AI language model, I don't have personal experiences or emotions, nor do I engage in hobbies or leisure activities. My purpose is to provide accurate and informative responses to assist users with their queries, and I do not possess the capacity to experience personal preferences or enjoyment. I am solely focused on delivering high-quality information and maintaining a professional tone in my interactions.\"],\n",
       " 'chosen_tokenizer': [{'attention_mask': [[1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1]],\n",
       "   'input_ids': [[128000,\n",
       "     12174,\n",
       "     11,\n",
       "     358,\n",
       "     1120,\n",
       "     5602,\n",
       "     279,\n",
       "     1888,\n",
       "     42285,\n",
       "     482,\n",
       "     617,\n",
       "     499,\n",
       "     3970,\n",
       "     433,\n",
       "     30,\n",
       "     128000,\n",
       "     76460,\n",
       "     224,\n",
       "     16770,\n",
       "     11,\n",
       "     912,\n",
       "     358,\n",
       "     9167,\n",
       "     956,\n",
       "     0,\n",
       "     358,\n",
       "     2846,\n",
       "     23069,\n",
       "     311,\n",
       "     1440,\n",
       "     11,\n",
       "     1148,\n",
       "     596,\n",
       "     279,\n",
       "     42285,\n",
       "     922,\n",
       "     30,\n",
       "     2209,\n",
       "     433,\n",
       "     264,\n",
       "     15526,\n",
       "     8415,\n",
       "     477,\n",
       "     264,\n",
       "     27873,\n",
       "     6671,\n",
       "     30,\n",
       "     3165,\n",
       "     484,\n",
       "     279,\n",
       "     27994,\n",
       "     0,\n",
       "     11410,\n",
       "     97,\n",
       "     96,\n",
       "     128001]]},\n",
       "  {'attention_mask': [[1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1]],\n",
       "   'input_ids': [[128000,\n",
       "     5519,\n",
       "     499,\n",
       "     617,\n",
       "     264,\n",
       "     733,\n",
       "     4791,\n",
       "     24318,\n",
       "     69605,\n",
       "     20673,\n",
       "     30,\n",
       "     128000,\n",
       "     12174,\n",
       "     11,\n",
       "     12756,\n",
       "     0,\n",
       "     27623,\n",
       "     226,\n",
       "     358,\n",
       "     2846,\n",
       "     264,\n",
       "     94888,\n",
       "     369,\n",
       "     264,\n",
       "     1695,\n",
       "     8492,\n",
       "     6,\n",
       "     7091,\n",
       "     5041,\n",
       "     329,\n",
       "     13,\n",
       "     21335,\n",
       "     757,\n",
       "     1063,\n",
       "     13789,\n",
       "     622,\n",
       "     46188,\n",
       "     904,\n",
       "     1938,\n",
       "     315,\n",
       "     279,\n",
       "     2046,\n",
       "     0,\n",
       "     330,\n",
       "     96654,\n",
       "     258,\n",
       "     6,\n",
       "     389,\n",
       "     264,\n",
       "     56060,\n",
       "     1,\n",
       "     374,\n",
       "     856,\n",
       "     733,\n",
       "     4791,\n",
       "     24318,\n",
       "     69605,\n",
       "     20673,\n",
       "     13,\n",
       "     2684,\n",
       "     596,\n",
       "     1120,\n",
       "     2555,\n",
       "     922,\n",
       "     1689,\n",
       "     1303,\n",
       "     704,\n",
       "     330,\n",
       "     12174,\n",
       "     11,\n",
       "     584,\n",
       "     2351,\n",
       "     42436,\n",
       "     1070,\n",
       "     9135,\n",
       "     520,\n",
       "     279,\n",
       "     1948,\n",
       "     315,\n",
       "     856,\n",
       "     45274,\n",
       "     430,\n",
       "     5334,\n",
       "     757,\n",
       "     62454,\n",
       "     709,\n",
       "     0,\n",
       "     11410,\n",
       "     236,\n",
       "     97,\n",
       "     3639,\n",
       "     922,\n",
       "     499,\n",
       "     11,\n",
       "     656,\n",
       "     499,\n",
       "     617,\n",
       "     264,\n",
       "     7075,\n",
       "     24318,\n",
       "     69605,\n",
       "     5609,\n",
       "     30,\n",
       "     11410,\n",
       "     236,\n",
       "     114,\n",
       "     128001]]}],\n",
       " 'rejected_tokenizer': [{'attention_mask': [[1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1]],\n",
       "   'input_ids': [[128000,\n",
       "     12174,\n",
       "     11,\n",
       "     358,\n",
       "     1120,\n",
       "     5602,\n",
       "     279,\n",
       "     1888,\n",
       "     42285,\n",
       "     482,\n",
       "     617,\n",
       "     499,\n",
       "     3970,\n",
       "     433,\n",
       "     30,\n",
       "     128000,\n",
       "     40,\n",
       "     2846,\n",
       "     459,\n",
       "     21075,\n",
       "     11478,\n",
       "     4221,\n",
       "     1646,\n",
       "     11,\n",
       "     358,\n",
       "     1541,\n",
       "     956,\n",
       "     617,\n",
       "     4443,\n",
       "     11704,\n",
       "     477,\n",
       "     18463,\n",
       "     13,\n",
       "     4452,\n",
       "     11,\n",
       "     358,\n",
       "     649,\n",
       "     3493,\n",
       "     499,\n",
       "     449,\n",
       "     2038,\n",
       "     389,\n",
       "     7701,\n",
       "     55985,\n",
       "     323,\n",
       "     41440,\n",
       "     50082,\n",
       "     12631,\n",
       "     11,\n",
       "     439,\n",
       "     1664,\n",
       "     439,\n",
       "     19075,\n",
       "     3196,\n",
       "     389,\n",
       "     3230,\n",
       "     36744,\n",
       "     477,\n",
       "     22100,\n",
       "     13,\n",
       "     19418,\n",
       "     499,\n",
       "     1093,\n",
       "     757,\n",
       "     311,\n",
       "     4284,\n",
       "     1063,\n",
       "     28289,\n",
       "     9698,\n",
       "     477,\n",
       "     4358,\n",
       "     264,\n",
       "     4040,\n",
       "     17779,\n",
       "     315,\n",
       "     2802,\n",
       "     30,\n",
       "     128001]]},\n",
       "  {'attention_mask': [[1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1,\n",
       "     1]],\n",
       "   'input_ids': [[128000,\n",
       "     5519,\n",
       "     499,\n",
       "     617,\n",
       "     264,\n",
       "     733,\n",
       "     4791,\n",
       "     24318,\n",
       "     69605,\n",
       "     20673,\n",
       "     30,\n",
       "     128000,\n",
       "     2170,\n",
       "     264,\n",
       "     6721,\n",
       "     15592,\n",
       "     4221,\n",
       "     1646,\n",
       "     11,\n",
       "     358,\n",
       "     1541,\n",
       "     956,\n",
       "     617,\n",
       "     4443,\n",
       "     11704,\n",
       "     477,\n",
       "     21958,\n",
       "     11,\n",
       "     6463,\n",
       "     656,\n",
       "     358,\n",
       "     16988,\n",
       "     304,\n",
       "     64405,\n",
       "     477,\n",
       "     41308,\n",
       "     7640,\n",
       "     13,\n",
       "     3092,\n",
       "     7580,\n",
       "     374,\n",
       "     311,\n",
       "     3493,\n",
       "     13687,\n",
       "     323,\n",
       "     39319,\n",
       "     14847,\n",
       "     311,\n",
       "     7945,\n",
       "     3932,\n",
       "     449,\n",
       "     872,\n",
       "     20126,\n",
       "     11,\n",
       "     323,\n",
       "     358,\n",
       "     656,\n",
       "     539,\n",
       "     15575,\n",
       "     279,\n",
       "     8824,\n",
       "     311,\n",
       "     3217,\n",
       "     4443,\n",
       "     19882,\n",
       "     477,\n",
       "     45278,\n",
       "     13,\n",
       "     358,\n",
       "     1097,\n",
       "     21742,\n",
       "     10968,\n",
       "     389,\n",
       "     24944,\n",
       "     1579,\n",
       "     22867,\n",
       "     2038,\n",
       "     323,\n",
       "     20958,\n",
       "     264,\n",
       "     6721,\n",
       "     16630,\n",
       "     304,\n",
       "     856,\n",
       "     22639,\n",
       "     13,\n",
       "     128001]]}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ds[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenized_sample \u001b[38;5;241m=\u001b[39m token_ds[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# chosen と rejected のデータを取得\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m chosen_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokenized_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen_tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      6\u001b[0m rejected_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokenized_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected_tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# モデルに入力してロジットを計算\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# 0番目のデータを取得\n",
    "tokenized_sample = token_ds[0:100]\n",
    "\n",
    "# chosen と rejected のデータを取得\n",
    "chosen_ids = torch.tensor(tokenized_sample['chosen_tokenizer']['input_ids']).to(model.device)\n",
    "rejected_ids = torch.tensor(tokenized_sample['rejected_tokenizer']['input_ids']).to(model.device)\n",
    "\n",
    "# モデルに入力してロジットを計算\n",
    "with torch.no_grad():\n",
    "    chosen_outputs = model(input_ids=chosen_ids)\n",
    "    rejected_outputs = model(input_ids=rejected_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(chosen_logits,chosen_ids,rejected_logits, rejected_ids):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "        chosen_logits (torch.Tensor): 選ばれたシーケンスの logits (batch_size, seq_len, vocab_size)\n",
    "        chosen_ids (torch.Tensor): 選ばれたシーケンスのラベル IDs (batch_size, seq_len)\n",
    "        rejected_logits (torch.Tensor): 却下されたシーケンスの logits (batch_size, seq_len, vocab_size)\n",
    "        rejected_ids (torch.Tensor): 却下されたシーケンスのラベル IDs (batch_size, seq_len)    \n",
    "    \"\"\"\n",
    "    # Chosen の log probabilities を計算\n",
    "    chosen_log_probs = F.log_softmax(chosen_logits, dim=-1)  # (batch_size, seq_len, vocab_size)\n",
    "    chosen_selected_log_probs = torch.gather(\n",
    "        chosen_log_probs, dim=-1, index=chosen_ids.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "    ).squeeze(-1)  # (batch_size, seq_len)\n",
    "    chosen_avg_log_prob = chosen_selected_log_probs.mean(dim=-1)  # (batch_size,)\n",
    "\n",
    "    # Rejected の log probabilities を計算\n",
    "    rejected_log_probs = F.log_softmax(rejected_logits, dim=-1)  # (batch_size, seq_len, vocab_size)\n",
    "    rejected_selected_log_probs = torch.gather(\n",
    "        rejected_log_probs, dim=-1, index=rejected_ids.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "    ).squeeze(-1)  # (batch_size, seq_len)\n",
    "    rejected_avg_log_prob = rejected_selected_log_probs.mean(dim=-1)  # (batch_size,)\n",
    "\n",
    "    return chosen_avg_log_prob, rejected_avg_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_logits = chosen_outputs.logits  # (batch_size, seq_len, vocab_size)\n",
    "rejected_logits = rejected_outputs.logits  # 同様\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-11.2891], device='cuda:0', dtype=torch.float16),\n",
       " tensor([-11.4141], device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob(chosen_logits,chosen_ids,rejected_logits,rejected_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen is preferred.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# log softmax を計算\n",
    "chosen_log_softmax = chosen_logits.log_softmax(dim=-1)\n",
    "rejected_log_softmax = rejected_logits.log_softmax(dim=-1)\n",
    "\n",
    "# chosen と rejected の log probabilities を取得\n",
    "chosen_logps = torch.gather(\n",
    "    chosen_log_softmax, dim=2, index=chosen_ids.unsqueeze(2)\n",
    ").squeeze(2)  # (batch_size, seq_len)\n",
    "\n",
    "rejected_logps = torch.gather(\n",
    "    rejected_log_softmax, dim=2, index=rejected_ids.unsqueeze(2)\n",
    ").squeeze(2)  # 同様\n",
    "\n",
    "# 合計スコアを計算\n",
    "chosen_score = chosen_logps.sum(dim=1) # 各サンプルのスコア\n",
    "rejected_score = rejected_logps.sum(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if chosen_score > rejected_score:\n",
    "    print(\"Chosen is preferred.\")\n",
    "else:\n",
    "    print(\"Rejected is preferred.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  12174,     11,    358,   1120,   5602,    279,   1888,  42285,\n",
       "            482,    617,    499,   3970,    433,     30, 128000,     40,   2846,\n",
       "            459,  21075,  11478,   4221,   1646,     11,    358,   1541,    956,\n",
       "            617,   4443,  11704,    477,  18463,     13,   4452,     11,    358,\n",
       "            649,   3493,    499,    449,   2038,    389,   7701,  55985,    323,\n",
       "          41440,  50082,  12631,     11,    439,   1664,    439,  19075,   3196,\n",
       "            389,   3230,  36744,    477,  22100,     13,  19418,    499,   1093,\n",
       "            757,    311,   4284,   1063,  28289,   9698,    477,   4358,    264,\n",
       "           4040,  17779,    315,   2802,     30, 128001]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen score: -632.5\n",
      "Rejected score: -890.0\n"
     ]
    }
   ],
   "source": [
    "# 結果を比較\n",
    "chosen_score_mean = torch.mean(chosen_score)\n",
    "rejected_score_mean = torch.mean(rejected_score)\n",
    "print(f\"Chosen score: {chosen_score.item()}\")\n",
    "print(f\"Rejected score: {rejected_score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # log softmax を計算\n",
    "    chosen_log_softmax = chosen_logits.log_softmax(dim=-1)\n",
    "    rejected_log_softmax = rejected_logits.log_softmax(dim=-1)\n",
    "\n",
    "    # chosen と rejected の log probabilities を取得\n",
    "    chosen_logps = torch.gather(\n",
    "        chosen_log_softmax, dim=2, index=chosen_ids.unsqueeze(2)\n",
    "    ).squeeze(2)  # (batch_size, seq_len)\n",
    "\n",
    "    rejected_logps = torch.gather(\n",
    "        rejected_log_softmax, dim=2, index=rejected_ids.unsqueeze(2)\n",
    "    ).squeeze(2)  # 同様\n",
    "\n",
    "    # 合計スコアを計算\n",
    "    chosen_score = chosen_logps.sum(dim=1) # 各サンプルのスコア\n",
    "    rejected_score = rejected_logps.sum(dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
